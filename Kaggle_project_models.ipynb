{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocesses**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. create the environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, Lasso, ElasticNetCV, LassoCV, Ridge\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import r2_score\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. load the training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames=pd.read_csv('ames_model_used.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1456, 55)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'MSZoning', 'LotArea', 'Alley', 'LotShape', 'LandContour',\n",
       "       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition_1', 'Condition_2',\n",
       "       'BldgType', 'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt',\n",
       "       'RoofStyle', 'RoofMatl', 'Exterior_1st', 'Exterior_2nd', 'MasVnrType',\n",
       "       'MasVnrArea', 'Foundation', 'BsmtExposure', 'BsmtFinType_1',\n",
       "       'BsmtFinType_2', 'TotalBsmtSF', 'Heating', 'HeatingQC', 'CentralAir',\n",
       "       'Electrical', 'GrLivArea', 'BedroomAbvGr', 'KitchenAbvGr',\n",
       "       'TotRmsAbvGrd', 'Fireplaces', 'GarageType', 'GarageFinish',\n",
       "       'GarageCars', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n",
       "       'EnclosedPorch', '_3SsnPorch', 'ScreenPorch', 'PoolArea', 'Fence',\n",
       "       'MiscFeature', 'MoSold', 'YrSold', 'SaleType', 'SaleCondition',\n",
       "       'SalePrice', 'YearSinceRemod', 'TotBathrooms'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ames.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1456, 54)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ames.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Split data into test and train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.log(ames.SalePrice) \n",
    "features=ames.drop(['SalePrice'],axis=1) \n",
    "X=pd.get_dummies(features, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,  random_state = 42, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1164, 195)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the evaluation function\n",
    "def evaluate(model, test_features, test_labels, train_features, train_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    errors = abs(predictions - test_labels)\n",
    "    mape = 100 * np.mean(errors / test_labels)\n",
    "    accuracy = 100 - mape\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    print('the goodness of fit r square for the train dateset is:',model.score(train_features, train_labels))\n",
    "    print('the goodness of fit r square for the test dateset is:',r2_score(test_labels, predictions))\n",
    "    print('the RMSE is: ', np.sqrt(mean_squared_error(test_labels, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Regression Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Average Error: 0.1002 degrees.\n",
      "Accuracy = 99.16%.\n",
      "the goodness of fit r square for the train dateset is: 0.9342271849011923\n",
      "the goodness of fit r square for the test dateset is: 0.7958213972545475\n",
      "the RMSE is:  0.18087503731015586\n"
     ]
    }
   ],
   "source": [
    "#create linear regression model\n",
    "lm=linear_model.LinearRegression()\n",
    "model=lm.fit(X_train, y_train)\n",
    "evaluate(model,X_test, y_test, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Regualization Models (Lasso and Ridge)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) Lasso**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = LassoCV(alphas = None, max_iter = 50000, cv = 10, normalize = True)\n",
    "lasso.fit(X_train,y_train)\n",
    "print('The Lasso lambda is:',lasso.alpha_)\n",
    "evaluate(lasso,X_test, y_test, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(lasso.predict(X_test) , y_test,marker = \"^\", label = \"Validation data\", s=9) \n",
    "plt.scatter(lasso.predict(X_train), y_train,marker = \".\", label = 'Training data', s=9)\n",
    "plt.title(\"Linear regression with Lasso regularization(Real Vs. Predicted)\")\n",
    "plt.xlabel(\"Predicted values\")\n",
    "plt.ylabel(\"Real Values\")\n",
    "plt.legend(loc = \"upper left\")\n",
    "plt.plot([10.5, 13.5], [10.5, 13.5], c = \"purple\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot feature importance (the magnitude of features in the regression funciton)\n",
    "coefs = pd.Series(lasso.coef_, index = X_train.columns)\n",
    "top_coefs = pd.concat([coefs.sort_values().head(10),\n",
    "                       coefs.sort_values().tail(10)])\n",
    "print(\"Lasso picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  \\\n",
    "      str(sum(coefs == 0)) + \" features\")\n",
    "top_coefs.plot(kind = 'barh')\n",
    "plt.title('Important Features')\n",
    "plt.xlim(-1,1)\n",
    "plt.xlabel('Coefficents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coefficient change with alpha\n",
    "lasso = linear_model.Lasso(normalize=True)\n",
    "alphas_lasso = np.logspace(-5, -1, 100)\n",
    "coef_lasso = [] #going to get one coef_ value for each alpha\n",
    "for i in alphas_lasso:\n",
    "    lasso.set_params(alpha=i).fit(X_train,y_train)\n",
    "    coef_lasso.append(lasso.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Lasso coefficients as a function of the regularization'\n",
    "columns = X_train.columns\n",
    "df_coef = pd.DataFrame(coef_lasso, index=alphas_lasso, columns=columns)\n",
    "df_coef.plot(logx=True, title=title, legend=False)\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('coefficients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Ridge**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_ridge = np.logspace(-5, 2, 100)\n",
    "ridge = RidgeCV(alphas = alphas_ridge, cv = 10, normalize = True)\n",
    "ridge.fit(X_train,y_train)\n",
    "print('The ridge lambda is:',ridge.alpha_)\n",
    "evaluate(ridge,X_test, y_test, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(ridge.predict(X_test) , y_test,marker = \"^\", label = \"Validation data\", s=9) \n",
    "plt.scatter(ridge.predict(X_train), y_train,marker = \".\", label = 'Training data', s=9)\n",
    "plt.title(\"Linear regression with Ridge regularization(Real Vs. Predicted)\")\n",
    "plt.xlabel(\"Predicted values\")\n",
    "plt.ylabel(\"Real Values\")\n",
    "plt.legend(loc = \"upper left\")\n",
    "plt.plot([10.5, 13.5], [10.5, 13.5], c = \"purple\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = pd.Series(ridge.coef_, index = X_train.columns)\n",
    "top_coefs = pd.concat([coefs.sort_values().head(10),\n",
    "                       coefs.sort_values().tail(10)])\n",
    "print(\"ridge picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  \\\n",
    "      str(sum(coefs == 0)) + \" features\")\n",
    "top_coefs.plot(kind = 'barh')\n",
    "plt.title('Important Features')\n",
    "plt.xlim(-1,1)\n",
    "plt.xlabel('Coefficents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coefficient change with alpha\n",
    "ridge = linear_model.Ridge(normalize=True)\n",
    "alphas_ridge = np.logspace(-3, 3, 100)\n",
    "coef_ridge = [] #going to get one coef_ value for each alpha\n",
    "for i in alphas_ridge:\n",
    "    ridge.set_params(alpha=i).fit(X_train,y_train)\n",
    "    coef_ridge.append(ridge.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Ridge coefficients as a function of the regularization'\n",
    "columns = X_train.columns\n",
    "df_coef = pd.DataFrame(coef_ridge, index=alphas_ridge, columns=columns)\n",
    "df_coef.plot(logx=True, title=title, legend=False)\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('coefficients')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) ElasticNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic = ElasticNetCV(alphas = None, max_iter = 50000, cv = 10, normalize = True)\n",
    "elastic.fit(X_train,y_train)\n",
    "print('The elastic lambda is:',elastic.alpha_)\n",
    "evaluate(elastic,X_test, y_test, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(elastic.predict(X_test) , y_test,marker = \"^\", label = \"Validation data\", s=9) \n",
    "plt.scatter(elastic.predict(X_train), y_train,marker = \".\", label = 'Training data', s=9)\n",
    "plt.title(\"Linear regression with ElasticNet regularization(Real Vs. Predicted)\")\n",
    "plt.xlabel(\"Predicted values\")\n",
    "plt.ylabel(\"Real Values\")\n",
    "plt.legend(loc = \"upper left\")\n",
    "plt.plot([10.5, 13.5], [10.5, 13.5], c = \"purple\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = pd.Series(elastic.coef_, index = X_train.columns)\n",
    "top_coefs = pd.concat([coefs.sort_values().head(10),\n",
    "                       coefs.sort_values().tail(10)])\n",
    "print(\"ElasticNet picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  \\\n",
    "      str(sum(coefs == 0)) + \" features\")\n",
    "top_coefs.plot(kind = 'barh')\n",
    "plt.title('Important Features')\n",
    "plt.xlim(-1,1)\n",
    "plt.xlabel('Coefficents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coefficient change with alpha\n",
    "elastic = linear_model.ElasticNet(normalize=True)\n",
    "alphas_elastic = np.logspace(-5, -1, 100)\n",
    "coef_elastic = [] #going to get one coef_ value for each alpha\n",
    "for i in alphas_elastic:\n",
    "    elastic.set_params(alpha=i).fit(X_train,y_train)\n",
    "    coef_elastic.append(elastic.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'ElasticNet coefficients as a function of alpha'\n",
    "columns = X_train.columns\n",
    "df_coef = pd.DataFrame(coef_elastic, index=alphas_elastic, columns=columns)\n",
    "df_coef.plot(logx=True, title=title, legend=False)\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('coefficients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Tree models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = ensemble.RandomForestRegressor(n_estimators = 100, random_state = 42, max_features=\n",
    "                                           150, max_depth=25)\n",
    "base_model.fit(X_train, y_train)\n",
    "base_accuracy = evaluate(base_model, X_test, y_test, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(base_model.predict(X_test) , y_test,marker = \"^\", label = \"Validation data\", s=9) \n",
    "plt.scatter(base_model.predict(X_train), y_train,marker = \".\", label = 'Training data', s=9)\n",
    "plt.title(\"Random Forest (Real Vs. Predicted)\")\n",
    "plt.xlabel(\"Predicted values\")\n",
    "plt.ylabel(\"Real Values\")\n",
    "plt.legend(loc = \"upper left\")\n",
    "plt.plot([10.5, 13.5], [10.5, 13.5], c = \"purple\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_base = list(zip(X.columns, base_model.feature_importances_))\n",
    "dtype = [('feature', 'S10'), ('importance', 'float')]\n",
    "feature_importance_base = np.array(feature_importance_base, dtype=dtype)\n",
    "feature_sort_base = np.sort(feature_importance_base, order='importance')[::-1]\n",
    "name, score = zip(*list(feature_sort_base))\n",
    "pd.DataFrame({'name':name,'score':score})[:15].sort_values('score').plot.barh(x='name', y='score', legend=None)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForest=ensemble.RandomForestRegressor()\n",
    "n_trees_range = range(30, 200, 10)  # OOB score will warning if too few trees\n",
    "train_error2 = []\n",
    "test_error2 = []\n",
    "oob_error = []\n",
    "\n",
    "for n_trees in n_trees_range:\n",
    "    randomForest.set_params(n_estimators=n_trees, random_state=42, oob_score=True)\n",
    "    randomForest.fit(X_train, y_train)\n",
    "    train_error2.append(1 - randomForest.score(X_train, y_train))\n",
    "    test_error2.append(1 - randomForest.score(X_test, y_test))\n",
    "    oob_error.append(1 - randomForest.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(n_trees_range, train_error2, c='red', label='training error')\n",
    "plt.plot(n_trees_range, test_error2, c='blue', label='test error')\n",
    "plt.plot(n_trees_range, oob_error, c='pink', label='oob error')\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Number of trees')\n",
    "plt.title('Errors as a function of number of trees')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) XGboost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor(learning_rate=0.08, max_depth=4, n_estimators=298)\n",
    "model.fit(X_train,y_train)\n",
    "XGboost_accuracy = evaluate(model, X_test, y_test, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor(learning_rate=0.08, max_depth=4, n_estimators=298)\n",
    "model.fit(X_train,y_train)\n",
    "preds = model.predict(X_test)\n",
    "np.sqrt(mean_squared_error(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = pd.DataFrame({'Variable':X_train.columns,\n",
    "              'Importance':model.feature_importances_}).sort_values('Importance', ascending=False)\n",
    "\n",
    "sns.barplot(x='Importance', y='Variable', data=imp.head(15))\n",
    "plt.title('Feature Importance XGBoost')\n",
    "plt.ylabel('Feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(model.predict(X_test) , y_test,marker = \"^\", label = \"Validation data\", s=9) \n",
    "plt.scatter(model.predict(X_train), y_train,marker = \".\", label = 'Training data', s=9)\n",
    "plt.title(\"XGboost (Real Vs. Predicted)\")\n",
    "plt.xlabel(\"Predicted values\")\n",
    "plt.ylabel(\"Real Values\")\n",
    "plt.legend(loc = \"upper left\")\n",
    "plt.plot([10.5, 13.5], [10.5, 13.5], c = \"purple\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "Kmodel = KNeighborsRegressor(algorithm='auto', leaf_size=15, n_neighbors=4, p=1, weights='distance')\n",
    "Kmodel.fit(rescaledX, y_train)\n",
    "XGboost_accuracy = evaluate(Kmodel, X_test, y_test, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. attempt at stacking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first model\n",
    "model1 = XGBRegressor(learning_rate=0.08, max_depth=4, n_estimators=298)\n",
    "\n",
    "\n",
    "model1.fit(X_train, y_train)\n",
    "\n",
    "test_pred1 = model1.predict(X_test)\n",
    "train_pred1 = model1.predict(X_train)\n",
    "\n",
    "\n",
    "train_pred1=pd.DataFrame(train_pred1)\n",
    "test_pred1=pd.DataFrame(test_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second model\n",
    "model2 = ensemble.RandomForestRegressor(bootstrap=True,\n",
    "criterion='mse',\n",
    "max_depth= None,\n",
    "max_features= 'auto',\n",
    "max_leaf_nodes= None,\n",
    "min_impurity_decrease= 0.0,\n",
    "min_impurity_split= None,\n",
    "min_samples_leaf= 1,\n",
    "min_samples_split= 2,\n",
    "min_weight_fraction_leaf= 0.0,\n",
    "n_estimators= 100,\n",
    "n_jobs= 1,\n",
    "oob_score= False,\n",
    "random_state= 42,\n",
    "verbose= 0,\n",
    "warm_start= False)\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "test_pred2 = model2.predict(X_test)\n",
    "train_pred2 = model2.predict(X_train)\n",
    "\n",
    "train_pred2=pd.DataFrame(train_pred2)\n",
    "test_pred2=pd.DataFrame(test_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stacking model1 and model2 and then using Ridge to train on results of previous two models\n",
    "df = pd.concat([train_pred1, train_pred2], axis=1)\n",
    "df_test = pd.concat([test_pred1, test_pred2], axis=1)\n",
    "\n",
    "model = Ridge(random_state=42)\n",
    "model.fit(df,y_train)\n",
    "model.score(df_test, y_test)\n",
    "preds = model.predict(df_test)\n",
    "np.sqrt(mean_squared_error(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
